COMPAS Audit — 300-word Summary

This audit used the COMPAS recidivism dataset to evaluate racial disparities in risk predictions. Using IBM AIF360 and a simple logistic regression classifier, we measured baseline differences in observed recidivism rates and post-prediction disparities. Key metrics computed include disparate impact (selection ratio), false positive rate (FPR) difference, and equal opportunity (true positive rate) difference.

Findings: The dataset shows differing base rates across racial groups; one group exhibited a higher average positive label rate than the other, indicating that underlying data reflects systemic differences in recorded outcomes. After training a logistic regression on standard COMPAS features and evaluating on a holdout set, the model produced a non-trivial FPR gap: the unprivileged group (as encoded in the dataset's `race` attribute) experienced higher false positive rates than the privileged group. Equal opportunity differences also indicated unequal true positive rates in some splits. Disparate impact measured on predicted labels confirmed the presence of predictive imbalance.

Remediation steps: (1) Pre-processing — apply reweighing or sample-balancing techniques to reduce label and feature bias before training. (2) In-processing — consider fairness-aware learning algorithms (e.g., adversarial debiasing, prejudice remover) that incorporate fairness constraints during training to directly optimize trade-offs between accuracy and fairness. (3) Post-processing — use calibrated equalized odds or threshold adjustments to align error rates across groups when retraining is infeasible. (4) Evaluation & governance — choose fairness metrics aligned with the deployment context (e.g., minimize FPR gap when false accusations are most harmful), run regular audits on fresh data, and log decisions to ensure accountability. (5) Human oversight — avoid relying solely on automated risk scores for high-stakes decisions; require human review and corroborating evidence.

Conclusion: The COMPAS audit highlights how models can amplify historical disparities. Combining technical mitigation with policy controls, transparency, and stakeholder engagement reduces harm and improves trust in deployed systems.